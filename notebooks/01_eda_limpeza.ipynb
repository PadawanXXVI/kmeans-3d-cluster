{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e8e0796",
   "metadata": {},
   "source": [
    "# 📘 Notebook 01 — EDA & Preparação do Dataset (Renda x Imóveis x Dívidas)\n",
    "\n",
    "Projeto: **Clusterização em 3D com K-Means**  \n",
    "Disciplina: **Aprendizado de Máquina Não Supervisionado** — Senac DF  \n",
    "Autores: **Anderson de Matos Guimarães, Renan Ost, Gustavo Stefano Thomazinho**\n",
    "\n",
    "**Objetivo deste notebook**  \n",
    "Explorar o dataset bruto de **Distribuição de Renda por Centis** (IRPF), entender o negócio, selecionar **3 variáveis contínuas** e produzir um dataset **limpo e padronizado** para o Notebook 02 (clusterização e visualização 3D)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f7808",
   "metadata": {},
   "source": [
    "## 🎯 Escopo & Entregáveis\n",
    "\n",
    "**Vamos:**\n",
    "1. Carregar e validar o CSV bruto.  \n",
    "2. Inspecionar colunas, tipos, nulos, duplicatas e consistência.  \n",
    "3. Documentar o **dicionário de dados** (visão de negócio).  \n",
    "4. Definir a **amostragem reprodutível** (300–500 linhas) com critérios claros.  \n",
    "5. Selecionar e preparar as **3 variáveis**:\n",
    "   - `rtb_soma_centil` (renda),\n",
    "   - `bens_imoveis` (patrimônio),\n",
    "   - `dividas_onus` (endividamento).\n",
    "6. Tratar outliers e zeros estruturais quando necessário (sem distorcer a realidade).  \n",
    "7. Escalonar (opcional) e **salvar** o dataset tratado (+ `metadata.json`).  \n",
    "\n",
    "**Saídas:**\n",
    "- `data/processed/distribuicao-renda-3vars.csv`  \n",
    "- `data/processed/distribuicao-renda-3vars.metadata.json`  \n",
    "- Gráficos e anotações que justificam as decisões."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a597719",
   "metadata": {},
   "source": [
    "## ✅ Critérios do Professor (como atendemos)\n",
    "\n",
    "- **Entradas (300–500)** → definiremos uma **amostra reprodutível** com base em (ano, entes federativos, centis).  \n",
    "- **Dados granulares** → centis (100 cortes por distribuição de RTB) garantem granularidade.  \n",
    "- **Numéricos contínuos** → valores monetários (R$) para as 3 variáveis.  \n",
    "- **Exatamente 3 variáveis** → renda, patrimônio (imóveis) e dívidas (3D pronto).  \n",
    "- **Notebook estilo IDEB** → manteremos seções claras, decisões justificadas e, no 02, poderemos comparar diferentes *k* (e opcionalmente usar dois anos se for relevante)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f618a79b",
   "metadata": {},
   "source": [
    "## 🗂️ Fonte de Dados, Licença e Paths\n",
    "\n",
    "- **Fonte oficial**: Receita Federal — Distribuição de Renda por Centis.  \n",
    "- **Arquivo bruto**: `data/raw/distribuicao-renda.csv`  \n",
    "- **Arquivo tratado (3 variáveis)**: `data/processed/distribuicao-renda-3vars.csv`\n",
    "\n",
    "> Observação: trabalharemos com unidades conforme o arquivo (muitos campos estão em **R$ milhões**). Faremos padronização de nomes e registraremos as unidades no metadado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b5fb6",
   "metadata": {},
   "source": [
    "## 🏢 Entendimento do Negócio (resumo)\n",
    "\n",
    "O dataset agrega informações de declarações de **IRPF** por **centis de renda tributável bruta (RTB)**.  \n",
    "Cada linha representa um **grupo** (centil) para um **ente federativo** em um **ano**.  \n",
    "As colunas trazem somatórios de rendimentos, bens/direitos, despesas dedutíveis, dívidas, etc.\n",
    "\n",
    "**Hipóteses de leitura econômica (para orientar a análise):**\n",
    "- `rtb_soma_centil` aproxima **capacidade de geração de renda** do grupo.  \n",
    "- `bens_imoveis` aproxima **acumulação patrimonial** estável.  \n",
    "- `dividas_onus` aproxima **alavancagem/endividamento**.  \n",
    "\n",
    "Essas três dimensões juntas formam um **espaço 3D** que deveria segregar perfis de grupos (baixa renda com baixa riqueza e baixa dívida vs. alta renda com alta riqueza e dívida variável, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb907d",
   "metadata": {},
   "source": [
    "## 📖 Dicionário de Dados (campos relevantes ao projeto)\n",
    "\n",
    "| Coluna original (exemplo)                 | Nome padronizado        | Tipo      | Unidade         | Observação de negócio |\n",
    "|-------------------------------------------|-------------------------|-----------|-----------------|-----------------------|\n",
    "| Ano-calendário                            | `ano`                   | int       | ano             | Ano da declaração     |\n",
    "| Ente Federativo                           | `uf`                    | string    | —               | Estado/Agregado       |\n",
    "| Centil                                    | `centil`                | float     | 1–100           | Corte por RTB         |\n",
    "| Rend. Trib. — Soma da RTB do Centil       | `rtb_soma_centil`       | float     | R$ milhões      | **Renda** (capacidade)|\n",
    "| Bens e Direitos — Imóveis                 | `bens_imoveis`          | float     | R$ milhões      | **Patrimônio**        |\n",
    "| Dívidas e Ônus                            | `dividas_onus`          | float     | R$ milhões      | **Endividamento**     |\n",
    "\n",
    "> Notas:\n",
    "> - Confirmaremos nomes exatos das colunas do CSV bruto e mapearemos para os padronizados acima.  \n",
    "> - Se necessário, convertendo vírgulas decimais e removendo formatações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, seed e paths (corrigidos para notebook em /notebooks)\n",
    "from __future__ import annotations\n",
    "import os, json, math, textwrap, re, sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.float_format\", lambda v: f\"{v:,.3f}\")\n",
    "\n",
    "# Paths\n",
    "ROOT = Path(\"..\").resolve()  # sobe um nível a partir de /notebooks\n",
    "DATA_RAW = ROOT / \"data\" / \"raw\" / \"distribuicao-renda.csv\"\n",
    "DATA_PROCESSED_DIR = ROOT / \"data\" / \"processed\"\n",
    "DATA_PROCESSED = DATA_PROCESSED_DIR / \"distribuicao-renda-3vars.csv\"\n",
    "METADATA = DATA_PROCESSED_DIR / \"distribuicao-renda-3vars.metadata.json\"\n",
    "FIG_DIR = ROOT / \"reports\" / \"figures\"\n",
    "for p in [DATA_PROCESSED_DIR, FIG_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FALLBACK_FILE = Path(\"/mnt/data/distribuicao-renda.csv\")\n",
    "if not DATA_RAW.exists() and FALLBACK_FILE.exists():\n",
    "    print(f\"[INFO] Usando fallback: {FALLBACK_FILE}\")\n",
    "    DATA_RAW = FALLBACK_FILE\n",
    "\n",
    "print(\"ROOT        :\", ROOT)\n",
    "print(\"DATA_RAW    :\", DATA_RAW)\n",
    "print(\"PROCESSED   :\", DATA_PROCESSED)\n",
    "print(\"FIG_DIR     :\", FIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_br(filepath: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Leitura robusta para CSV com possíveis variações:\n",
    "    - separador ',' ou ';'\n",
    "    - decimal '.' ou ','\n",
    "    - encoding 'utf-8' ou 'latin-1'\n",
    "    Retorna o DataFrame lido com detecção automática.\n",
    "    \"\"\"\n",
    "    trials = [\n",
    "        dict(sep=\";\", decimal=\",\", encoding=\"utf-8\", engine=\"python\"),\n",
    "        dict(sep=\",\", decimal=\",\", encoding=\"utf-8\", engine=\"python\"),\n",
    "        dict(sep=\";\", decimal=\".\", encoding=\"utf-8\", engine=\"python\"),\n",
    "        dict(sep=\",\", decimal=\".\", encoding=\"utf-8\", engine=\"python\"),\n",
    "        dict(sep=\";\", decimal=\",\", encoding=\"latin-1\", engine=\"python\"),\n",
    "        dict(sep=\",\", decimal=\",\", encoding=\"latin-1\", engine=\"python\"),\n",
    "    ]\n",
    "    last_err = None\n",
    "    for opts in trials:\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, **opts)\n",
    "            if df.shape[1] == 1:\n",
    "                last_err = RuntimeError(\"provável separador incorreto (1 coluna)\")\n",
    "                continue\n",
    "            print(f\"[OK] Leitura com parâmetros: {opts}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Falha ao ler {filepath}: {last_err}\")\n",
    "\n",
    "def snake(s: str) -> str:\n",
    "    s2 = re.sub(r\"[^\\w]+\", \"_\", s.strip().lower(), flags=re.UNICODE)\n",
    "    s2 = re.sub(r\"_{2,}\", \"_\", s2).strip(\"_\")\n",
    "    return s2\n",
    "\n",
    "def find_col(candidates: List[str], patterns: List[str]) -> str | None:\n",
    "    for c in candidates:\n",
    "        cl = c.lower()\n",
    "        ok = True\n",
    "        for pat in patterns:\n",
    "            ors = pat.split(\"|\")\n",
    "            if not any(o in cl for o in ors):\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def quantiles_report(s: pd.Series, qs=(0.5, 0.9, 0.95, 0.99)) -> pd.Series:\n",
    "    qv = s.quantile(q=list(qs))\n",
    "    qv.index = [f\"q{int(q*100):02d}\" for q in qs]\n",
    "    return qv\n",
    "\n",
    "def savefig(path: Path):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=160)\n",
    "    print(f\"[FIG] salvo: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59b7d2",
   "metadata": {},
   "source": [
    "## 🔍 EDA do Arquivo Bruto\n",
    "\n",
    "Nesta seção faremos:\n",
    "1. **Leitura segura** (encoding, separador, decimal).  \n",
    "2. **Shape, colunas, tipos, nulos, duplicatas**.  \n",
    "3. **Estatísticas descritivas** (mediana, p95, p99) para entender caudas.  \n",
    "4. **Sanidade de chaves lógicas**: (ano, uf, centil) sem duplicidades por registro.  \n",
    "5. **Distribuições**:\n",
    "   - Histogramas de `rtb_soma_centil`, `bens_imoveis`, `dividas_onus`.  \n",
    "   - Scatterpairs para relações bivariadas.  \n",
    "6. **Zeros e “ausências esperadas”**: bens ou dívidas podem ter zeros; não confundir com *missing*.  \n",
    "7. **Outliers**: avaliar se são fenômeno real (altíssima concentração no topo) → provavelmente **não remover**, apenas documentar e considerar **log-transform** opcional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = read_csv_br(DATA_RAW)\n",
    "\n",
    "print(\"\\n# VISÃO GERAL\")\n",
    "print(\"shape:\", df_raw.shape)\n",
    "display(df_raw.head(10))\n",
    "\n",
    "print(\"\\n# COLUNAS\")\n",
    "print(list(df_raw.columns))\n",
    "\n",
    "print(\"\\n# TIPOS\")\n",
    "display(df_raw.dtypes)\n",
    "\n",
    "print(\"\\n# NULOS (%)\")\n",
    "display((df_raw.isna().mean() * 100).round(2).sort_values(ascending=False))\n",
    "\n",
    "print(\"\\n# Duplicatas (linhas idênticas):\", df_raw.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df_raw.columns)\n",
    "\n",
    "col_ano    = find_col(cols, [\"ano\", \"calendario|calendário\"])\n",
    "col_uf     = find_col(cols, [\"ente|uf|federativo\"])\n",
    "col_centil = find_col(cols, [\"centil\"])\n",
    "\n",
    "col_rtb_soma   = find_col(cols, [\"rendimentos|rtb\", \"soma|somatorio|somatório\"])\n",
    "col_bens_imov  = find_col(cols, [\"bens|direitos\", \"imoveis|imóveis\"])\n",
    "col_dividas    = find_col(cols, [\"dividas|dívidas\", \"onus|ônus\"])\n",
    "\n",
    "mapping = {\n",
    "    col_ano: \"ano\",\n",
    "    col_uf: \"uf\",\n",
    "    col_centil: \"centil\",\n",
    "    col_rtb_soma: \"rtb_soma_centil\",\n",
    "    col_bens_imov: \"bens_imoveis\",\n",
    "    col_dividas: \"dividas_onus\",\n",
    "}\n",
    "\n",
    "print(\"# MAPEAMENTO PROPOSTO\")\n",
    "display(mapping)\n",
    "\n",
    "missing_keys = [k for k in mapping if k is None]\n",
    "if missing_keys:\n",
    "    raise ValueError(\"Não consegui detectar automaticamente algumas colunas. Revise os padrões desta célula e rode novamente.\")\n",
    "\n",
    "df = df_raw.rename(columns={k: v for k, v in mapping.items()})\n",
    "df.columns = [snake(c) for c in df.columns]\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c580124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Tipagem robusta ===\n",
    "df[\"ano\"] = (\n",
    "    df[\"ano\"].astype(str).str.extract(r\"(\\d{4})\", expand=False)\n",
    "      .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "      .astype(\"Int64\")\n",
    ")\n",
    "\n",
    "centil_raw = (\n",
    "    df[\"centil\"]\n",
    "      .astype(str)\n",
    "      .str.replace(\",\", \".\", regex=False)\n",
    "      .str.extract(r\"(\\d+(?:\\.\\d+)?)\", expand=False)\n",
    ")\n",
    "df[\"centil\"] = pd.to_numeric(centil_raw, errors=\"coerce\")\n",
    "\n",
    "for c in [\"rtb_soma_centil\", \"bens_imoveis\", \"dividas_onus\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"rtb_soma_centil\", \"bens_imoveis\", \"dividas_onus\"])\n",
    "\n",
    "mask_centil = (df[\"centil\"] >= 1) & (df[\"centil\"] <= 100)\n",
    "rem_out = (~mask_centil).sum()\n",
    "if rem_out:\n",
    "    print(f\"[INFO] Removendo {rem_out} linhas com centil fora de [1,100]\")\n",
    "df = df[mask_centil].copy()\n",
    "\n",
    "dup = df.duplicated(subset=[\"ano\", \"uf\", \"centil\"]).sum()\n",
    "print(\"Duplicidades em (ano, uf, centil):\", dup)\n",
    "\n",
    "print(\"Shape após limpeza básica:\", df.shape)\n",
    "display(df[[\"ano\", \"uf\", \"centil\", \"rtb_soma_centil\", \"bens_imoveis\", \"dividas_onus\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0356068",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"rtb_soma_centil\", \"bens_imoveis\", \"dividas_onus\"]\n",
    "display(df[num_cols].describe().T)\n",
    "\n",
    "qtab = pd.concat([quantiles_report(df[c]) for c in num_cols], axis=1).T\n",
    "qtab.columns = qtab.columns.str.upper()\n",
    "display(qtab)\n",
    "\n",
    "print(\"anos:\", sorted(df[\"ano\"].dropna().unique().tolist())[:10], \"...\")\n",
    "print(\"UFs (amostra):\", df[\"uf\"].dropna().unique()[:10])\n",
    "print(\"linhas totais:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21334d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3.6))\n",
    "for i,c in enumerate(num_cols, 1):\n",
    "    plt.subplot(1,3,i)\n",
    "    plt.hist(df[c].dropna(), bins=40)\n",
    "    plt.title(c)\n",
    "    plt.xlabel(\"valor\")\n",
    "    plt.ylabel(\"freq\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "pairs = [(\"rtb_soma_centil\",\"bens_imoveis\"),\n",
    "         (\"rtb_soma_centil\",\"dividas_onus\"),\n",
    "         (\"bens_imoveis\",\"dividas_onus\")]\n",
    "\n",
    "plt.figure(figsize=(12,3.6))\n",
    "for i,(x,y) in enumerate(pairs,1):\n",
    "    plt.subplot(1,3,i)\n",
    "    plt.scatter(df[x], df[y], s=8, alpha=0.6)\n",
    "    plt.xlabel(x); plt.ylabel(y)\n",
    "    plt.title(f\"{x} vs {y}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for c in num_cols:\n",
    "    df[f\"log_{c}\"] = np.log1p(df[c].clip(lower=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f51509",
   "metadata": {},
   "source": [
    "## 🧪 Estratégia de Amostragem (300–500 linhas)\n",
    "\n",
    "**Princípio**: reprodutibilidade + representatividade.\n",
    "\n",
    "**Passos**:\n",
    "1. Escolher **um ano-base** (tipicamente o mais recente disponível).  \n",
    "2. Calcular o total de linhas (UF × centis) e **estimar** quantos UFs e centis precisamos para cair entre **300–500**.  \n",
    "3. Estratégias possíveis (a definir após ver o shape real):\n",
    "   - **E1 (recomendada)**: Fixar ano; **usar todos os UFs**; selecionar um **intervalo contínuo de centis** (ex.: 1–20, 30–60, 90–100) que dê ~300–500.  \n",
    "   - **E2**: Fixar ano; **amostrar UFs** (estratificado por região) e usar **todos os centis** desses UFs.  \n",
    "4. Fixar uma **seed** para qualquer amostragem aleatória.  \n",
    "5. Registrar a regra no `metadata.json`.\n",
    "\n",
    "> Justificativa didática: manter **centis contíguos** preserva estrutura da distribuição e facilita interpretar clusters (baixa, média, alta renda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22699b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_year_base(data: pd.DataFrame) -> int:\n",
    "    anos = data[\"ano\"].dropna().astype(int)\n",
    "    year = int(anos.max())\n",
    "    print(f\"[ANO-BASE] Selecionado automaticamente: {year}\")\n",
    "    return year\n",
    "\n",
    "def sample_rows(data: pd.DataFrame, target_min: int = 300, target_max: int = 500) -> pd.DataFrame:\n",
    "    year = choose_year_base(data)\n",
    "    dfy = data.query(\"ano == @year\").copy()\n",
    "    possiveis_agregados = {\"brasil\", \"nacional\", \"todos\", \"agregado\"}\n",
    "    dfy[\"uf_lc\"] = dfy[\"uf\"].astype(str).str.lower()\n",
    "    dfy = dfy[~dfy[\"uf_lc\"].isin(possiveis_agregados)].drop(columns=[\"uf_lc\"])\n",
    "\n",
    "    n_uf = dfy[\"uf\"].nunique()\n",
    "    print(f\"[AMOSTRA] UFs distintos no ano {year}: {n_uf}\")\n",
    "\n",
    "    N_min = math.ceil(target_min / n_uf)\n",
    "    N_max = min(100, math.floor(target_max / n_uf))\n",
    "    N = max(1, min(N_max, max(N_min, 10)))\n",
    "    print(f\"[AMOSTRA] Intervalo de centis: 1..{N} (alvo {target_min}-{target_max})\")\n",
    "\n",
    "    df_sample = dfy[dfy[\"centil\"].between(1, N, inclusive=\"both\")].copy()\n",
    "    print(f\"[AMOSTRA] Linhas resultantes: {len(df_sample)}\")\n",
    "    return df_sample\n",
    "\n",
    "df_sample = sample_rows(df)\n",
    "display(df_sample.head())\n",
    "display(df_sample.tail())\n",
    "print(\"shape:\", df_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f0b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = df_sample.duplicated(subset=[\"ano\", \"uf\", \"centil\"]).sum()\n",
    "print(\"Duplicidades (ano,uf,centil) na amostra:\", dups)\n",
    "\n",
    "print(\"Nulos nas variáveis selecionadas:\")\n",
    "display(df_sample[[\"rtb_soma_centil\", \"bens_imoveis\", \"dividas_onus\"]].isna().sum())\n",
    "\n",
    "display(df_sample[[\"rtb_soma_centil\", \"bens_imoveis\", \"dividas_onus\"]].describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a68a8c8",
   "metadata": {},
   "source": [
    "## 🧼 Limpeza & Transformações\n",
    "\n",
    "1. **Padronizar nomes** de colunas para *snake_case*.  \n",
    "2. **Selecionar colunas**: `ano`, `uf`, `centil`, `rtb_soma_centil`, `bens_imoveis`, `dividas_onus`.  \n",
    "3. **Tipos corretos** (int/float); tratar decimal com vírgula, se houver.  \n",
    "4. **Checagens de integridade**:\n",
    "   - `centil` ∈ [1, 100]  \n",
    "   - Sem duplicidade para (ano, uf, centil) na amostra.  \n",
    "5. **Tratamento de escalas**:\n",
    "   - Manter valores em **R$ milhões** (consistência com a fonte).  \n",
    "   - Criar **versão log-transform** para visualização, se necessário (`log1p`).  \n",
    "6. **Escalonamento (para o Notebook 02)**:\n",
    "   - Salvar **dados crus** e, opcionalmente, uma **cópia escalada** (StandardScaler/MinMax) para K-Means.  \n",
    "   - A decisão final de escalonamento será aplicada no Notebook 02; aqui apenas deixamos a função e um preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684cce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_final = [\"ano\", \"uf\", \"centil\", \"rtb_soma_centil\", \"bens_imoveis\", \"dividas_onus\"]\n",
    "df_out = df_sample[cols_final].copy()\n",
    "\n",
    "# Salvar\n",
    "DATA_PROCESSED.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(DATA_PROCESSED, index=False)\n",
    "print(f\"[SALVO] {DATA_PROCESSED} ({len(df_out)} linhas)\")\n",
    "\n",
    "# Metadados\n",
    "metadata = {\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"source_file\": str(DATA_RAW),\n",
    "    \"output_file\": str(DATA_PROCESSED),\n",
    "    \"year_base\": int(df_out[\"ano\"].dropna().max()) if len(df_out) else None,\n",
    "    \"sampling_rule\": \"ano=max; UFs=all (excl. agregados nacionais); centis=1..N tal que linhas entre 300-500\",\n",
    "    \"n_rows\": int(len(df_out)),\n",
    "    \"units\": {\n",
    "        \"rtb_soma_centil\": \"R$ milhões (somatório por centil)\",\n",
    "        \"bens_imoveis\": \"R$ milhões (somatório por UF-centil)\",\n",
    "        \"dividas_onus\": \"R$ milhões (somatório por UF-centil)\",\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"Colunas padronizadas para snake_case.\",\n",
    "        \"Zeros podem representar ausência real de bens/dívidas.\",\n",
    "        \"EDA completa salva em reports/figures/.\",\n",
    "        \"Transformações log(1+x) usadas apenas para visualização na EDA.\",\n",
    "    ],\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"authors\": [\n",
    "        \"Anderson de Matos Guimarães\",\n",
    "        \"Renan Ost\",\n",
    "        \"Gustavo Stefano Thomazinho\",\n",
    "    ],\n",
    "}\n",
    "import json\n",
    "with open(METADATA, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[SALVO] {METADATA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "X = df_out[[\"rtb_soma_centil\", \"bens_imoveis\", \"dividas_onus\"]].values\n",
    "\n",
    "sc_std = StandardScaler()\n",
    "X_std = sc_std.fit_transform(X)\n",
    "\n",
    "sc_mm = MinMaxScaler()\n",
    "X_mm = sc_mm.fit_transform(X)\n",
    "\n",
    "print(\"Preview StandardScaler (primeiras 5 linhas):\")\n",
    "print(pd.DataFrame(X_std, columns=[\"rtb_std\",\"imoveis_std\",\"dividas_std\"]).head())\n",
    "\n",
    "print(\"\\nPreview MinMaxScaler (primeiras 5 linhas):\")\n",
    "print(pd.DataFrame(X_mm, columns=[\"rtb_mm\",\"imoveis_mm\",\"dividas_mm\"]).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dde25e",
   "metadata": {},
   "source": [
    "## 💾 Persistência dos Resultados\n",
    "\n",
    "- **CSV final**: `data/processed/distribuicao-renda-3vars.csv`  \n",
    "  - Colunas: `ano, uf, centil, rtb_soma_centil, bens_imoveis, dividas_onus`  \n",
    "  - Somente linhas da **amostra definida**.  \n",
    "- **Metadados**: `data/processed/distribuicao-renda-3vars.metadata.json`  \n",
    "  - `created_at`, `source_file`, `year_base`, `sampling_rule`, `n_rows`, `units`, `notes`.  \n",
    "- **Imagens** (opcional): `reports/figures/eda_*`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb7769a",
   "metadata": {},
   "source": [
    "## 🧰 Reprodutibilidade e Versão\n",
    "\n",
    "- Scripts utilitários em `src/utils.py` (funções de leitura, checagens, gráficos rápidos).  \n",
    "- Fixar `RANDOM_SEED` no topo do notebook.  \n",
    "- Salvar `pip freeze` (opcional) em `requirements.txt` (já existe no repo).  \n",
    "- Comentar decisões no corpo do notebook para facilitar a correção."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d774f7a7",
   "metadata": {},
   "source": [
    "## ⚖️ Limitações & Ética\n",
    "\n",
    "- Dados **agregados** por centis (não individuais).  \n",
    "- Possível **assimetria extrema** nos top centis (riqueza concentrada).  \n",
    "- “Zero” em patrimônio/dívida pode indicar **ausência real**, não erro.  \n",
    "- Interpretações devem ser **econômicas** e **contextualizadas** (não normativas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf89e7b",
   "metadata": {},
   "source": [
    "## 🗺️ Próximos Passos (Notebook 02)\n",
    "\n",
    "- Escolha do **k** (Elbow, Silhouette).  \n",
    "- **K-Means** com dados escalados.  \n",
    "- **Gráfico 3D interativo** (Plotly) dos clusters.  \n",
    "- **Animação** (Formação dos clusters / frames por iteração ou por *k*).  \n",
    "- Interpretação dos grupos e relato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec73b9",
   "metadata": {},
   "source": [
    "## ✅ Checklist (para eu mesmo)\n",
    "\n",
    "- [ ] CSV bruto carregado e validado  \n",
    "- [ ] Dicionário de dados preenchido com nomes exatos  \n",
    "- [ ] Estratégia de amostragem definida e aplicada  \n",
    "- [ ] 3 variáveis selecionadas e conferidas  \n",
    "- [ ] CSV tratado salvo + metadados gerados  \n",
    "- [ ] Gráficos EDA salvos em `reports/figures/`  \n",
    "- [ ] Commit com mensagem padrão **conventional commits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ebd536",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks = {\n",
    "    \"csv_tratado_existe\": DATA_PROCESSED.exists(),\n",
    "    \"metadata_existe\": METADATA.exists(),\n",
    "    \"linhas_entre_300_500\": (300 <= len(pd.read_csv(DATA_PROCESSED)) <= 500) if DATA_PROCESSED.exists() else False,\n",
    "    \"tem_colunas_certas\": (set(pd.read_csv(DATA_PROCESSED).columns) == {\"ano\", \"uf\", \"centil\", \"rtb_soma_centil\", \"bens_imoveis\", \"dividas_onus\"}) if DATA_PROCESSED.exists() else False,\n",
    "}\n",
    "print(checks)\n",
    "print(\"[OK] Todos os checks passaram.\" if all(checks.values()) else \"[ATENÇÃO] Algum check falhou.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
